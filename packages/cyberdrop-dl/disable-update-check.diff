diff --git c/cyberdrop_dl/main.py w/cyberdrop_dl/main.py
index 4c7e62b5f92f..c7a7708be7b3 100644
--- c/cyberdrop_dl/main.py
+++ w/cyberdrop_dl/main.py
@@ -25,7 +25,7 @@ from cyberdrop_dl.utils.apprise import send_apprise_notifications
 from cyberdrop_dl.utils.dumper import Dumper
 from cyberdrop_dl.utils.logger import RedactedConsole, add_custom_log_render, log, log_spacer, log_with_color
 from cyberdrop_dl.utils.sorting import Sorter
-from cyberdrop_dl.utils.updates import check_latest_pypi
+# from cyberdrop_dl.utils.updates import check_latest_pypi
 from cyberdrop_dl.utils.utilities import check_partials_and_empty_folders, send_webhook_message
 from cyberdrop_dl.utils.yaml import handle_validation_error
 
@@ -284,7 +284,7 @@ async def director(manager: Manager) -> None:
         if not configs_to_run:
             log_spacer(20)
             log("Checking for Updates...", 20)
-            check_latest_pypi()
+            # check_latest_pypi()
             log_spacer(20)
             log("Closing Program...", 20)
             log_with_color("Finished downloading. Enjoy :)", "green", 20, show_in_stats=False)
diff --git c/cyberdrop_dl/scraper/__init__.py w/cyberdrop_dl/scraper/__init__.py
index 2035580f038e..e543970650ef 100644
--- c/cyberdrop_dl/scraper/__init__.py
+++ w/cyberdrop_dl/scraper/__init__.py
@@ -54,7 +54,6 @@ from cyberdrop_dl.scraper.crawlers.rule34xyz_crawler import Rule34XYZCrawler
 from cyberdrop_dl.scraper.crawlers.saint_crawler import SaintCrawler
 from cyberdrop_dl.scraper.crawlers.scrolller_crawler import ScrolllerCrawler
 from cyberdrop_dl.scraper.crawlers.sendvid_crawler import SendVidCrawler
-from cyberdrop_dl.scraper.crawlers.sex_dot_com_crawler import SexDotComCrawler
 from cyberdrop_dl.scraper.crawlers.simpcity_crawler import SimpCityCrawler
 from cyberdrop_dl.scraper.crawlers.socialmediagirls_crawler import SocialMediaGirlsCrawler
 from cyberdrop_dl.scraper.crawlers.spankbang_crawler import SpankBangCrawler
diff --git c/cyberdrop_dl/scraper/crawlers/sex_dot_com_crawler.py w/cyberdrop_dl/scraper/crawlers/sex_dot_com_crawler.py
deleted file mode 100644
index 38e9a0cac64c..000000000000
--- c/cyberdrop_dl/scraper/crawlers/sex_dot_com_crawler.py
+++ /dev/null
@@ -1,125 +0,0 @@
-from __future__ import annotations
-
-import calendar
-from typing import TYPE_CHECKING
-
-from dateutil import parser
-from yarl import URL
-
-from cyberdrop_dl.scraper.crawler import Crawler, create_task_id
-from cyberdrop_dl.utils.data_enums_classes.url_objects import FILE_HOST_PROFILE, ScrapeItem
-from cyberdrop_dl.utils.utilities import error_handling_wrapper, get_filename_and_ext
-
-if TYPE_CHECKING:
-    from collections.abc import AsyncGenerator
-
-    from cyberdrop_dl.managers.manager import Manager
-
-
-class SexDotComCrawler(Crawler):
-    primary_base_domain = URL("https://sex.com")
-
-    def __init__(self, manager: Manager) -> None:
-        super().__init__(manager, "sex", "Sex.com")
-        self.api_url = URL("https://iframe.sex.com/api/")
-
-    """~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"""
-
-    @create_task_id
-    async def fetch(self, scrape_item: ScrapeItem) -> None:
-        """Determines where to send the scrape item based on the url."""
-        parts = scrape_item.url.parts
-        if len(parts) > 5:
-            await self.post(scrape_item)
-        elif len(parts) <= 5 and parts[2] == "shorts":
-            await self.profile(scrape_item)
-
-    async def shorts_profile_paginator(self, scrape_item: ScrapeItem) -> AsyncGenerator[dict]:
-        username = scrape_item.url.parts[3]
-        page = 1
-        while True:
-            posts_api_url = self.api_url / "feed" / "listUserItems"
-
-            query = {
-                "pageSize": 300,
-                "pageNumber": page,
-                "visibility": "public",
-                "username": username,
-            }
-
-            posts_api_url = posts_api_url.with_query(query)
-            async with self.request_limiter:
-                json_data = await self.client.get_json(self.domain, posts_api_url, origin=scrape_item)
-
-            if scrape_item.album_id is None:
-                user_id = json_data["page"]["items"][0]["media"]["user"]["userUid"]
-                scrape_item.album_id = user_id
-                scrape_item.add_to_parent_title(self.create_title(username))
-
-            yield json_data
-
-            if not json_data["page"]["pageInfo"]["hasNextPage"]:
-                break
-            page += 1
-
-    @error_handling_wrapper
-    async def get_media(self, scrape_item: ScrapeItem) -> dict:
-        """Gets media from its relative URL."""
-        relative_url = "/".join(scrape_item.url.parts[3:])
-        data_url = self.api_url / "media" / "getMedia"
-        query = {
-            "relativeUrl": relative_url,
-        }
-        data_url = data_url.with_query(query)
-        async with self.request_limiter:
-            json_data = await self.client.get_json(self.domain, data_url, origin=scrape_item)
-        return json_data["media"]
-
-    @error_handling_wrapper
-    async def handle_media(self, scrape_item: ScrapeItem, item: dict | None) -> None:
-        if item is None:
-            item = await self.get_media(scrape_item)
-        relative_url = item["relativeUrl"]
-        canonical_url = URL("https://sex.com/en/shorts") / relative_url
-        if await self.check_complete_from_referer(canonical_url):
-            return
-
-        fileType = item.get("fileType") or item.get("mediaType")
-        if fileType.startswith("image"):
-            media_url = URL(item["fullPath"]).with_query(
-                {
-                    "optimizer": "image",
-                    "width": 1200,
-                }
-            )
-            filename = f"{item['pictureUid']}.jpg"
-            ext = "jpg"
-        elif fileType.startswith("video"):
-            media_url = URL(item["sources"][0]["fullPath"])
-            filename, ext = get_filename_and_ext(media_url.name)
-        date = self.parse_datetime(item["createdAt"])
-        new_scrape_item = self.create_scrape_item(scrape_item, canonical_url, "", True, scrape_item.album_id, date)
-        await self.handle_file(media_url, new_scrape_item, filename, ext)
-        scrape_item.add_children()
-
-    @error_handling_wrapper
-    async def profile(self, scrape_item: ScrapeItem) -> None:
-        """Scrapes shorts."""
-        scrape_item.set_type(FILE_HOST_PROFILE, self.manager)
-        async for json_data in self.shorts_profile_paginator(scrape_item):
-            for item in json_data["page"]["items"]:
-                await self.handle_media(scrape_item, item["media"])
-
-    async def post(self, scrape_item: ScrapeItem):
-        """Scrapes a post."""
-        username = scrape_item.url.parts[2]
-        scrape_item.add_to_parent_title(self.create_title(username))
-        await self.handle_media(scrape_item, None)
-
-    """~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"""
-
-    @staticmethod
-    def parse_datetime(date: str) -> int:
-        """Parses a datetime string into a unix timestamp."""
-        parsed_date = parser.isoparse(date)
-        return calendar.timegm(parsed_date.utctimetuple())
diff --git c/cyberdrop_dl/ui/program_ui.py w/cyberdrop_dl/ui/program_ui.py
index 4864dc9ad1c2..8ca086695556 100644
--- c/cyberdrop_dl/ui/program_ui.py
+++ w/cyberdrop_dl/ui/program_ui.py
@@ -19,7 +19,7 @@ from cyberdrop_dl.ui.prompts.basic_prompts import ask_dir_path, enter_to_continu
 from cyberdrop_dl.ui.prompts.defaults import DONE_CHOICE, EXIT_CHOICE
 from cyberdrop_dl.utils.cookie_management import clear_cookies
 from cyberdrop_dl.utils.sorting import Sorter
-from cyberdrop_dl.utils.updates import check_latest_pypi
+# from cyberdrop_dl.utils.updates import check_latest_pypi
 from cyberdrop_dl.utils.utilities import clear_term, open_in_text_editor
 
 if TYPE_CHECKING:
@@ -104,7 +104,7 @@ class ProgramUI:
 
     def _check_updates(self) -> None:
         """Checks Cyberdrop-DL updates."""
-        check_latest_pypi(logging="CONSOLE")
+        # check_latest_pypi(logging="CONSOLE")
         enter_to_continue()
 
     def _change_config(self) -> None:
@@ -267,7 +267,7 @@ class ProgramUI:
         """Get latest changelog file from github. Returns its content."""
         path = self.manager.path_manager.config_folder.parent / "CHANGELOG.md"
         url = "https://raw.githubusercontent.com/jbsparrow/CyberDropDownloader/refs/heads/master/CHANGELOG.md"
-        _, latest_version = check_latest_pypi(logging="OFF")
+        _, latest_version = None
         if not latest_version:
             self.print_error("UNABLE TO GET LATEST VERSION INFORMATION")
             return None
